---
title: "Homework Quiz"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    df_print: paged
    css: ../../../styles.css
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<br><br>

1. I want to predict how well 6 year-olds are going to do in their final school exams. Using the following variables am I likely under-fitting, fitting well or over-fitting? Postcode, gender, reading level, score in maths test, date of birth, family income.

Without seeing the actual data it is hard to say. However, it will probably generate too many variables and over-fit the model.


2. If I have two models, one with an AIC score of 34,902 and the other with an AIC score of 33,559 which model should I use?

The one with the lowest score (33,559)


3. I have two models, the first with: r-squared: 0.44, adjusted r-squared: 0.43. The second with: r-squared: 0.47, adjusted r-squared: 0.41. Which one should I use?

If it is a MLR model, the first. If it is a single LR model, the second.


4. I have a model with the following errors: RMSE error on test set: 10.3, RMSE error on training data: 10.4. Do you think this model is over-fitting?

When the error is higher on predicting training data it means that the model is over-fitting.


5. How does k-fold validation work?

It consists of dividing the sample in k subsamples and performing cross validation among them, i.e. comparing error between the k-th sample (that becomes the test set) and the rest of the samples (that become the train set).


6. What is a validation set? When do you need one?

It is a set that not used to test or train, as an extra layer of protection against over-fitting.

7. Describe how backwards selection works.

Start with the over-fitted model then deselect features until the resulting model is well-fit.

8. Describe how best subset selection works.

At each step of adding/subtracting features from a model, compare all possible feature combinations.







